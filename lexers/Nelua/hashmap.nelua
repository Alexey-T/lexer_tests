--[[
The hashmap library provides a hash table with fixed types.

A hash map is an associative container that contains key-value pairs with unique keys.
Search, insertion, and removal of elements have average constant-time complexity.

The hash map share similarities with Lua tables but should not be used like them,
the main differences are:
 * There is no array part.
 * The length operator returns number of elements in the map.
 * Indexing automatically inserts a key-value pair, to avoid this use `peek()` method.
 * Values cannot be `nil` or set to `nil`.
 * Can only use `pairs()` to iterate.

Any failure when growing a hash map raises an error.
]]

require 'memory'
require 'hash'
require 'span'

-- Ceil integer division.
local function ceilidiv(x: usize, y: usize): usize <inline>
  return (x + y - 1) // y
end

-- Hash modulo operation, `n` must always be a power of 2.
local function hashmod(h: usize, n: usize): usize <inline>
  -- the following would be faster but expects good hash of `h`
  -- return h & (n - 1)
  return h % ((n - 1) | 1)
end

-- Compute the smallest power of 2 not smaller than `n`.
local function roundpow2(n: usize): usize <inline>
  if n & (n - 1) == 0 then return n end
  n = n | (n >> 1)
  n = n | (n >> 2)
  n = n | (n >> 4)
  n = n | (n >> 8)
  n = n | (n >> 16)
  ## if primtypes.usize.size > 4 then -- usize has more than 32 bits
    n = n | (n >> 32)
  ## end
  n = n + 1
  return n
end

-- Maximum load factor (number of elements per bucket) in percent.
-- The container automatically increases the number of buckets if the load factor exceeds this threshold.
local MAX_LOAD_FACTOR: usize <comptime> = 75
-- Grow rate in percent.
-- When the maximum load factor is reached the container capacity grows by this factor.
local GROW_RATE: usize <comptime> = 200
-- Initial bucket capacity to reserve when inserting an element for the first time in a container.
local INIT_CAPACITY: usize <comptime> = 8
-- Constant used to test invalid index.
local INVALID_INDEX: usize <comptime> = (@usize)(-1)

## local function make_hashmapT(K, V, HashFunc, KeyEqualFunc, Allocator)
  ## static_assert(traits.is_type(K), "invalid type '%s'", K)
  ## static_assert(traits.is_type(V), "invalid type '%s'", V)
  ## if not Allocator then
  require 'allocators.default'
  ## Allocator = DefaultAllocator
  ## end

  local Allocator: type = #[Allocator]#
  local K: type = @#[K]#
  local V: type = @#[V]#

  -- Hash map node record defined when instantiating the generic `hashmap`.
  local hashnodeT: type <nickname(#[string.format('hashmapnode(%s, %s)',K,V)]#)> = @record{
    key: K,
    value: V,
    filled: boolean,
    next: usize,
  }

  -- Hash map record defined when instantiating the generic `hashmap`.
  local hashmapT: type <nickname(#[string.format('hashmap(%s, %s)',K,V)]#)> = @record{
    buckets: span(usize),
    nodes: span(hashnodeT),
    size: usize,
    free_index: usize,
    allocator: Allocator
  }

  ##[[
  local hashmapT = hashmapT.value
  hashmapT.is_hashmap = true
  hashmapT.is_container = true
  hashmapT.K = K
  hashmapT.V = V
  ]]

  --[[
  Creates a hash map using a custom allocator instance.
  Useful only when using instanced allocators.
  ]]
  function hashmapT.make(allocator: Allocator): hashmapT
    local m: hashmapT
    m.allocator = allocator
    return m
  end

  --[[
  Resets the container to a zeroed state, freeing all used resources.

  *Complexity*: O(1).
  ]]
  function hashmapT:destroy(): void
    self.allocator:spandealloc(self.buckets)
    self.allocator:spandealloc(self.nodes)
    self.buckets = (@span(usize))()
    self.nodes = (@span(hashnodeT))()
    self.size = 0
    self.free_index = 0
  end

  -- Effectively the same as `destroy`, called when a to-be-closed variable goes out of scope.
  function hashmapT:__close(): void
    self:destroy()
  end

  --[[
  Remove all elements from the container.
  The internal storage buffers are not freed, and they may be reused.

  *Complexity*: O(n).
  ]]
  function hashmapT:clear(): void
    self.size = 0
    memory.spanset(self.buckets, INVALID_INDEX)
    -- link free nodes
    memory.spanzero(self.nodes)
    local free_index: usize = INVALID_INDEX
    for i:isize=(@isize)(self.nodes.size-1),0,-1 do
      self.nodes[(@usize)(i)].next = free_index
      free_index = (@usize)(i)
    end
    self.free_index = free_index
  end

  -- Used internally to find a value at a key returning it's node index.
  function hashmapT:_find(key: K): (usize, usize, usize) <inline>
    ## if HashFunc then
    local h: usize = (@usize)(#[HashFunc]#(key))
    ## else
    local h: usize = hash.hash(key)
    ## end
    local bucket_index: usize = hashmod(h, self.buckets.size)
    local prev_node_index: usize = INVALID_INDEX
    if unlikely(self.buckets.size == 0) then -- container is empty
      return INVALID_INDEX, prev_node_index, bucket_index
    end
    local node_index: usize = self.buckets[bucket_index]
    -- iterate until the key is found
    while node_index ~= INVALID_INDEX do
      local node: *hashnodeT = &self.nodes[node_index]
      ## if KeyEqualFunc then
      local eq: boolean = #[KeyEqualFunc]#(key, node.key)
      ## elseif K.is_record and K.metafields.__keyequal then
      local eq: boolean = key:__keyequal(node.key)
      ## else
      local eq: boolean = key == node.key
      ## end
      if eq then
        return node_index, prev_node_index, bucket_index
      end
      prev_node_index = node_index
      node_index = node.next
    end
    return node_index, prev_node_index, bucket_index
  end

  --[[
  Sets the number of buckets to at least `bucket_count` and rehashes the container when needed.
  The number of new buckets will always be at least
  the smallest appropriate value to not exceed the maximum load factor,
  thus rehashing with 0 `bucket_count` can be used to shrink the hash map.

  Rehash invalidates all references to element values previously returned.

  *Complexity*: Average case O(n).
  ]]
  function hashmapT:rehash(bucket_count: usize): void <noinline>
    -- buckets count should be at least (size * 100) / MAX_LOAD_FACTOR
    local min_buckets_count: usize = ceilidiv(self.size * 100, MAX_LOAD_FACTOR)
    if bucket_count < min_buckets_count then
      bucket_count = min_buckets_count
    end
    bucket_count = roundpow2(bucket_count)
    -- node count must have at least one extra free node
    local node_count: usize = ceilidiv(bucket_count * MAX_LOAD_FACTOR, 100)
    if bucket_count > 0 and node_count <= self.size then
      node_count = self.size + 1
    end
    -- shift filled nodes when shrinking
    if node_count < self.nodes.size and self.nodes.size > 0 and node_count > 0 then
      local j: usize = 0
      while j < self.nodes.size and self.nodes[j].filled do j = j + 1 end
      for i:usize=j,<self.nodes.size do
        if self.nodes[i].filled then
          self.nodes[j] = self.nodes[i]
          j = j + 1
        end
      end
      for i:usize=j,<self.nodes.size do
        self.nodes[i] = (@hashnodeT)()
      end
      check(j == self.size)
    end
    -- reallocate nodes and buckets
    self.nodes = self.allocator:xspanrealloc0(self.nodes, node_count)
    self.buckets = self.allocator:xspanrealloc(self.buckets, bucket_count)
    memory.spanset(self.buckets, INVALID_INDEX)
    -- unlink used nodes while linking free nodes
    local free_index: usize = INVALID_INDEX
    for i:isize=(@isize)(self.nodes.size-1),0,-1 do
      local node: *hashnodeT = &self.nodes[(@usize)(i)]
      if node.filled then
        node.next = INVALID_INDEX
      else
        node.next = free_index
        free_index = (@usize)(i)
      end
    end
    self.free_index = free_index
    -- fill buckets and link used nodes
    for i:usize=0,<self.nodes.size do
      if self.nodes[i].filled then
        local node_index: usize, prev_node_index: usize, bucket_index: usize = self:_find(self.nodes[i].key)
        if likely(prev_node_index == INVALID_INDEX) then
          self.buckets[bucket_index] = i
        else
          self.nodes[prev_node_index].next = i
        end
        self.nodes[i].next = node_index
      end
    end
  end

  --[[
  Sets the number of buckets to the number needed to accommodate at least `count` elements
  without exceeding maximum load factor and rehashes the container when needed.

  *Complexity*: Average case O(n).
  ]]
  function hashmapT:reserve(count: usize): void
    local bucket_count: usize = ceilidiv(count * 100, MAX_LOAD_FACTOR)
    if bucket_count > self.buckets.size then
      self:rehash(bucket_count)
    end
  end

  -- Used internally to find or make a value at a key returning it's node index.
  function hashmapT:_at(key: K): usize
    if unlikely(self.buckets.size == 0) then -- no buckets, initialize it
      self:rehash(INIT_CAPACITY)
    end
    local node_index: usize, prev_node_index: usize, bucket_index: usize = self:_find(key)
    if node_index ~= INVALID_INDEX then -- found a node
      return node_index
    else -- add a node
      -- used a free node
      local node_index: usize = self.free_index
      check(node_index < self.nodes.size, 'not enough space to add a node')
      local node: *hashnodeT = &self.nodes[node_index]
      self.free_index = node.next
      -- link the new node
      $node = {key = key, filled = true, next = INVALID_INDEX}
      if likely(prev_node_index == INVALID_INDEX) then
        self.buckets[bucket_index] = node_index
      else
        self.nodes[prev_node_index].next = node_index
      end
      self.size = self.size + 1
      -- allocate more space when hitting the max load factor
      if unlikely(self.size * 100 >= self.buckets.size * MAX_LOAD_FACTOR) then
        self:rehash(ceilidiv(self.size * GROW_RATE, MAX_LOAD_FACTOR))
      end
      return node_index
    end
  end

  --[[
  Returns a reference to the value that is mapped to a key.
  If such key does not exist, then it's inserted and a rehash may happen.
  The reference will remain valid until next rehash (when growing).
  This allows indexing the hash map with square brackets `[]`.

  *Complexity*: Average case O(1).
  ]]
  function hashmapT:__atindex(key: K): *V
    return &self.nodes[self:_at(key)].value
  end

  --[[
  Returns a reference to the value that is mapped to a key.
  If no such element exists, returns `nilptr`.
  The reference will remain valid until next rehash (when growing).

  *Complexity*: Average case O(1).
  ]]
  function hashmapT:peek(key: K): *V
    local node_index: usize = self:_find(key)
    if node_index ~= INVALID_INDEX then
      return &self.nodes[node_index].value
    end
    return nilptr
  end

  --[[
  Returns true if a key exists in the container.

  *Complexity*: Average case O(1).
  ]]
  function hashmapT:has(key: K): boolean
    return self:peek(key) ~= nilptr
  end

  --[[
  Returns true plus the value for the element with key `key` in in the container in case it exists.
  Otherwise, returns false plus a zero initialized element.

  *Complexity*: Average case O(1).
  ]]
  function hashmapT:has_and_get(key: K): (boolean, V)
    local value: *V = self:peek(key)
    if value == nilptr then return false, V() end
    return true, $value
  end

  --[[
  Removes an element with a key from the container (if it exists).
  Returns the removed value that was was actually removed.
  If the key does not exist, then returns a zeroed value.

  It's safe to remove an element while iterating.
  References to element values previously returned will remain valid.

  *Complexity*: Average case O(1).
  ]]
  function hashmapT:remove(key: K): V
    local node_index: usize, prev_node_index: usize, bucket_index: usize = self:_find(key)
    if unlikely(node_index == INVALID_INDEX) then return V() end
    -- unlink the removed node
    local node: *hashnodeT = &self.nodes[node_index]
    local value: V = node.value
    if likely(prev_node_index == INVALID_INDEX) then
      self.buckets[bucket_index] = node.next
    else
      self.nodes[prev_node_index].next = node.next
    end
    self.size = self.size - 1
    -- link free node
    $node = (@hashnodeT){next = self.free_index}
    self.free_index = node_index
    return value
  end

  -- Returns the average number of elements per bucket.
  function hashmapT:loadfactor(): number
    if unlikely(self.buckets.size == 0) then
      return 0
    else
      return self.size / self.buckets.size
    end
  end

  -- Returns the number of buckets in the container.
  function hashmapT:bucketcount(): usize
    return self.buckets.size
  end

  -- Returns the number of elements the container can store before triggering a rehash.
  function hashmapT:capacity(): usize
    return self.nodes.size
  end

  -- Returns the number of elements in the container.
  function hashmapT:__len(): isize
    return (@isize)(self.size)
  end

  -- Hash map iterator.
  local hashmap_iteratorT: type = @record{
    container: *hashmapT,
    index: usize
  }

  -- Used internally by iterator `next` and `mnext`.
  function hashmap_iteratorT:_next_node(key: K): *hashnodeT <inline>
    if unlikely(self.index == INVALID_INDEX) then
      self.index = 0
    else
      self.index = self.index + 1
    end
    while self.index < self.container.nodes.size do
      local node: *hashnodeT = &self.container.nodes[self.index]
      if node.filled then
        return node
      end
      self.index = self.index + 1
    end
    return nilptr
  end

  --[[
  Advances the container iterator returning its key and value.

  *Remarks*: The input `key` is actually ignored.
  ]]
  function hashmap_iteratorT:next(key: K): (boolean, K, V) <inline>
    local node: *hashnodeT = self:_next_node(key)
    if not node then return false, (@K)(), (@V)() end
    return true, node.key, node.value
  end

  --[[
  Advances the container iterator returning its key and value by reference.

  *Remarks*: The input `key` is actually ignored.
  ]]
  function hashmap_iteratorT:mnext(key: K): (boolean, K, *V) <inline>
    local node: *hashnodeT = self:_next_node(key)
    if not node then return false, (@K)(), nilptr end
    return true, node.key, &node.value
  end

  -- Allow using `pairs()` to iterate the container.
  function hashmapT:__pairs(): (auto, hashmap_iteratorT, K) <inline>
    return hashmap_iteratorT.next, (@hashmap_iteratorT){container=self,index=INVALID_INDEX}, (@K)()
  end

  -- Allow using `mpairs()` to iterate the container.
  function hashmapT:__mpairs(): (auto, hashmap_iteratorT, K) <inline>
    return hashmap_iteratorT.mnext, (@hashmap_iteratorT){container=self,index=INVALID_INDEX}, (@K)()
  end

  -- Used internally by `__next` and `__mnext`.
  function hashmapT:_next_node(key: facultative(K)): *hashnodeT <inline>
    ## if key.type.is_niltype then
    local next_node_index: usize = 0
    ## else
    local node_index: usize = self:_find(key)
    -- TODO: next should actually work in case of removed keys (like in Lua)
    assert(node_index ~= INVALID_INDEX, 'attempt to use next for an invalid key in hashmap')
    local next_node_index: usize = node_index + 1
    ## end
    while next_node_index < self.nodes.size do
      local node: *hashnodeT = &self.nodes[next_node_index]
      if node.filled then
        return node
      end
      next_node_index = next_node_index + 1
    end
    return nilptr
  end

  -- Allow using `next()` to iterate the container.
  function hashmapT:__next(key: facultative(K)): (boolean, K, V)
    local node: *hashnodeT = self:_next_node(key)
    if not node then return false, (@K)(), (@V)() end
    return true, node.key, node.value
  end

  -- Allow using `mnext()` to iterate the container.
  function hashmapT:__mnext(key: facultative(K)): (boolean, K, *V)
    local node: *hashnodeT = self:_next_node(key)
    if not node then return false, (@K)(), nilptr end
    return true, node.key, &node.value
  end

  ## return hashmapT
## end

--[[
Generic used to instantiate a hash map type in the form of `hashmap(K, V, HashFunc, KeyEqualFunc, Allocator)`.

Argument `K` is the key type for the hash map.
Argument `V` is the value type for the hash map.
Argument `HashFunc` is a function to hash a key,
in case absent then `hash.hash` is used.
Argument `KeyEqualFunc` is a function to compare two keys,
in case absent then `==` is used.
Argument `Allocator` is an allocator type for the container storage,
in case absent then then `DefaultAllocator` is used.
]]
global hashmap: type = #[generalize(make_hashmapT)]#

return hashmap
