--[[
This is a minimal general purpose heap allocator, that could serve as replacement to
the system's general allocator. It requires a pre allocated memory region in advance.
It's purpose is to have predictable allocation and deallocation time
when you can allocate the maximum memory usage in advance.

It uses linked lists to search for the best free node.
It tries to have a fast alloc/dealloc.
However it may fragment more than other allocators.

In some cases it can be faster than the general purpose allocator.
However usually you are better off with the system's general purpose allocator.
This may be more useful to have reliable alloc/dealloc time on real time applications,
or if you want to avoid the system's default allocator for some reason,
or if the system does not have an allocator.

Its memory cannot grow automatically, use the system's general purpose allocator for that.
The allocator is not thread safe, it was designed to be used in single thread applications.
Allocations are always aligned to the platform max alignment, typically 16 bytes.

*NOTE*: This is experimental, a bunch of tests were done but is not really battle tested.

The implementation is based on
[this project](https://github.com/CCareaga/heap_allocator),
however it has heavy customized to have more performance, constant time allocations
and alignment.
]]

require 'memory'

-- Do not change this alignment, the implementation have this in mind.
local ALLOC_ALIGN: usize <comptime> = #[typedefs.maxalign]#
-- The minimum allocation size for joining and splitting chunks.
local MIN_ALLOC_SIZE: usize <comptime> = 16
--[[
Number of bin for different chunks sizes,
lowering this may increase performance at cost of more fragmentation when doing big allocations.
24 is set to work well with at most 64MB in a single allocation.
]]
local BIN_COUNT <comptime> = 24
--[[
Number of maximum lookups per bin when searching for a free chunk.
This is necessary to break long loops in alloc for edge cases.
]]
local BIN_MAX_LOOKUPS <comptime> = 16
-- Cookie used to identify invalid realloc/dealloc on invalid pointers.
local NODE_COOKIE: usize <comptime> = (@usize)(0xA7512BCF)

--[[
Each chunk have this node as heading before its data.
This node will be 32 bytes on 64bit systems,
or 16 bytes on 32bit systems, perfect for the 16 byte alignment requirement.
]]
local HeapNode: type = @record{
  size: usize,
  prev_adj: *HeapNode,
  next: *HeapNode,
  prev: *HeapNode
}

-- The double linked list for free nodes.
local Bin: type = @record{
  head: *HeapNode
}

-- The heap stores bins for different chunk sizes.
local Heap: type = @record{
  bins: [BIN_COUNT]Bin
}

function HeapNode:set_used(): void
  --[[
  the lower bits of the pointer should be always 0 because we allocate with alignment,
  thus it's impossible to have conflicts in next/prev with these values due to alignment
  ]]
  self.next = (@*HeapNode)(1_usize)
  self.prev = (@*HeapNode)(NODE_COOKIE)
end

function HeapNode:is_used(): boolean
  return self.next == (@*HeapNode)(1_usize) and self.prev == (@*HeapNode)(NODE_COOKIE)
end

-- Efficient clz (compute leading zeros) from C, used to compute the bin.
local function clz(x: uint32): uint32 <inline,nosideeffect>
##[==[ cemit([[
#if defined(__GNUC__) && (__GNUC__ >= 4)
  x = __builtin_clz(x);
#else
  static const int MultiplyDeBruijnBitPosition[32] = {
    0, 9, 1, 10, 13, 21, 2, 29, 11, 14, 16, 18, 22, 25, 3, 30,
    8, 12, 20, 28, 15, 17, 24, 7, 19, 27, 23, 6, 26, 5, 4, 31
  };
  x |= x >> 1;
  x |= x >> 2;
  x |= x >> 4;
  x |= x >> 8;
  x |= x >> 16;
  x = 31 - MultiplyDeBruijnBitPosition[(uint32_t)(x * 0x07C4ACDDU) >> 27];
#endif
]])
]==]
  return x
end

--[[
Hashing function that converts allocation size into bin index.
Changing this function will change the binning policy of the heap.
]]
local function get_bin_index(size: usize): uint32 <inline>
  if unlikely(size <= (1<<3)) then
    return 0
  elseif unlikely(size >= 1<<(3+(BIN_COUNT<<0))) then
    return BIN_COUNT - 1
  else
    return (28 - clz((@uint32)(size)))>>0
  end
end

-- Aligns an address.
local function align_forward(addr: usize, align: usize): usize <inline>
  return (addr + (align-1)) & ~(align-1)
end

-- Inserts a node inside the bin linked list.
function Heap:add_node(node: *HeapNode): void
  local bin: *Bin = &self.bins[get_bin_index(node.size)]
  node.prev = nilptr
  if likely(bin.head ~= nilptr) then
    -- bin is not empty, forward the head
    node.next = bin.head
    bin.head.prev = node
  else
    node.next = nilptr
  end
  bin.head = node
end

-- Removes a node from the bin linked list.
function Heap:remove_bin_node(bin_index: uint32, node: *HeapNode): void
  local bin: *Bin = &self.bins[bin_index]
  -- update the head in case we are removing it
  if node == bin.head then
    bin.head = node.next
  end
  -- link prev node
  if node.prev ~= nilptr then
    node.prev.next = node.next
  end
  -- link next node
  if node.next ~= nilptr then
    node.next.prev = node.prev
  end
end

-- Removes a node from then bin linked list.
function Heap:remove_node(node: *HeapNode): void
  self:remove_bin_node(get_bin_index(node.size), node)
end

-- Returns next adjacent node.
local function get_next_adj_node(node: *HeapNode): *HeapNode
  return (@*HeapNode)((@usize)(node) + #@HeapNode + node.size)
end

-- Gets a node given a pointer.
local function get_ptr_node(p: pointer): *HeapNode
  -- check for misaligned invalid pointers
  if unlikely((@usize)(p) & (ALLOC_ALIGN-1) ~= 0) then
    return nilptr
  end
  -- the actual head of the node is not p, it is p minus the size of the node
  local node: *HeapNode = (@*HeapNode)((@usize)(p) - #@HeapNode)
  -- check if is a valid allocator node
  if unlikely(not node:is_used()) then
    return nilptr
  end
  return node
end

-- Adds more memory to the heap.
function Heap:add_memory_region(region: pointer, region_size: usize): void
  local region_start: usize = (@usize)(region)

  -- we want to start the heap aligned
  local heap_start: usize = align_forward(region_start, ALLOC_ALIGN)

  -- calculate the offset for the heap
  local region_offset: usize = heap_start - region_start
  check(region_size >= region_offset + #@HeapNode, 'heap region size is too small')

  -- the heap size is the region size minus alignment offset and ending node space
  local heap_size: usize = region_size - region_offset - #@HeapNode

  -- first we create the initial region
  -- the heap starts as just one big chunk of allocable memory
  local start_node: *HeapNode = (@*HeapNode)(heap_start)
  start_node.size = heap_size - #@HeapNode -- this node itself use some metadata space
  start_node.prev_adj = nilptr
  start_node.next = nilptr
  start_node.prev = nilptr

  -- set the ending node
  local end_node: *HeapNode = (@*HeapNode)(heap_start + heap_size)
  end_node.size = 0
  end_node.prev_adj = start_node
  end_node:set_used() -- the end node is never free

  -- now we add the region to the correct bin
  self:add_node(start_node)
end

--[[
This is the allocation function of the heap.
It takes the size of the chunk.
It will search through the bins until it finds a suitable chunk,
and then split the chunk if necessary returning the start of the chunk.
]]
function Heap:alloc(size: usize): pointer
  -- ignore 0 size allocations
  if unlikely(size == 0) then return nilptr end

  -- we always want to allocate aligned sizes
  size = align_forward(size + #@HeapNode, ALLOC_ALIGN) - #@HeapNode

  local found: *HeapNode

  -- advance through the bins until we find a chunk that fits the size
  local bin_index: uint32 = get_bin_index(size)
  repeat
    found = self.bins[bin_index].head
    -- limit the number of max lookups here to advancete to next bin early
    -- thus allocating faster
    for i:uint32=0,<BIN_MAX_LOOKUPS do
      if found == nilptr then
        break
      elseif found.size >= size then
        -- found a free chunk!
        goto found_free_node
      end
      found = found.next
    end
    bin_index = bin_index + 1
  until bin_index == BIN_COUNT

  -- this is rare, maybe there is free memory available however it could be too fragmented
  -- so try to search again without limits
  bin_index = get_bin_index(size)
  repeat
    found = self.bins[bin_index].head
    while found ~= nilptr do
      if found.size >= size then
        goto found_free_node
      end
      found = found.next
    end
    bin_index = bin_index + 1
  until bin_index == BIN_COUNT

  -- no free chunk found, out of memory
  do return nilptr end

::found_free_node::
  -- if the difference between the found chunk and the requested chunk
  -- is bigger than the metadata + min alloc size
  -- then we should split this chunk, otherwise just return the chunk
  if found.size > size + (#@HeapNode + MIN_ALLOC_SIZE) then
    -- do the math to get where to split at, then set its metadata
    local split_size: usize = found.size - size - #@HeapNode
    found.size = size
    local split: *HeapNode = get_next_adj_node(found)
    split.size = split_size

    -- update adjacent links
    split.prev_adj = found
    get_next_adj_node(split).prev_adj = split

    -- add it in the correct bin
    self:add_node(split)
  end

  self:remove_bin_node(bin_index, found) -- remove it from its bin
  found:set_used() -- not free anymore

  -- return the pointer for the chunk, it should be properly aligned
  return (@pointer)((@usize)(found) + #@HeapNode)
end

--[[
This is the deallocation function of the heap.
It takes the heap record pointer and the pointer provided by the `heap_alloc` function.
The given chunk will be possibly coalesced and then placed in the correct bin.
]]
function Heap:dealloc(p: pointer): void
  -- ignore nil pointers
  if unlikely(p == nilptr) then return end

  -- the actual head of the node is not p, it is p minus the size of the node
  local head: *HeapNode = get_ptr_node(p)
  if unlikely(head == nilptr) then
    panic('invalid pointer passed in heap dealloc')
  end

  local prev: *HeapNode = head.prev_adj
  local next: *HeapNode

  -- if the previous node is free we can coalesce!
  if likely(prev ~= nilptr) and not prev:is_used() then
    -- remove the previous node from its bin
    self:remove_node(prev)

    -- re-calculate the size of this node and link next adjacent node
    prev.size = prev.size + #@HeapNode + head.size
    next = get_next_adj_node(prev)
    next.prev_adj = prev

    -- invalidate the head contents
    -- in case user tries to deallocate this pointer again (double free)
    head.next = nilptr
    head.prev = nilptr

    -- previous is now the node we are working with, we head to prev
    -- because the next if statement will coalesce with the next node
    -- and we want that statement to work even when we coalesce with prev
    head = prev
  else
    next = get_next_adj_node(head)
  end

  -- if the next node is free coalesce!
  if not next:is_used() then
    -- remove it from its bin
    self:remove_node(next)

    -- re-calculate the new size of head
    head.size = head.size + #@HeapNode + next.size

    -- link next adjacent node
    get_next_adj_node(head).prev_adj = head
  end

  -- this chunk is now free, so put it in the right bin
  self:add_node(head)
end

-- This is the reallocation function of the heap.
function Heap:realloc(p: pointer, size: usize): pointer
  if unlikely(p == nilptr) then
    -- no previous allocation, just alloc
    return self:alloc(size)
  elseif unlikely(size == 0) then
    -- the new size is 0, just dealloc
    self:dealloc(p)
    return nilptr
  end

  -- the actual head of the node is not p, it is p minus the size of the node
  local head: *HeapNode = get_ptr_node(p)
  if unlikely(head == nilptr) then
    panic('invalid pointer passed in heap realloc')
  end

  -- we always want to allocate aligned sizes
  size = align_forward(size + #@HeapNode, ALLOC_ALIGN) - #@HeapNode

  -- is the chunk growing?
  if likely(size > head.size) then
    -- we can only grow if the next adjacent node
    -- is not the end, is free and has enough space
    local next: *HeapNode = get_next_adj_node(head)
    if not next:is_used() and head.size + next.size + #@HeapNode >= size then
      -- remove it from its bin
      self:remove_node(next)

      -- re-calculate the new size of head
      head.size = head.size + next.size + #@HeapNode

      -- link next adjacent node
      get_next_adj_node(head).prev_adj = head

      -- the chunk is now merged with a larger chunk
      -- however it may be shrinked yet
    else
      -- the next node has not enough space,
      -- we need to allocate a new chunk and move data there
      local newp: pointer = self:alloc(size)
      if newp == nilptr then
        -- out of memory, cancel the realloc
        return nilptr
      end
      memory.copy(newp, p, head.size)
      self:dealloc(p)
      return newp
    end
  end

  -- do we need to shrink the chunk?
  if head.size > size then
    -- it's only useful to shrink if the freed size
    -- is bigger than the metadata + min alloc size
    if head.size > size + (#@HeapNode + MIN_ALLOC_SIZE) then
      -- do the math to get where to split at, then set its metadata
      local split_size: usize = head.size - size - #@HeapNode
      head.size = size
      local split: *HeapNode = get_next_adj_node(head)
      split.size = split_size

      -- update adjacent links
      split.prev_adj = head
      get_next_adj_node(split).prev_adj = split

      -- now we need to get the new index for this split chunk
      -- place it in the correct bin
      self:add_node(split)
    end
  end

  return p
end

## local function make_HeapAllocatorT(SIZE)
  ## static_assert(traits.is_number(SIZE), 'size must be a number')

  local SIZE: usize <comptime> = #[SIZE]#

  -- Heap allocator record defined when instantiating the generic `HeapAllocator`.
  local HeapAllocatorT: type = @record{
    initialized: boolean,
    heap: Heap,
    buffer: [SIZE]byte
  }

  -- Initializes the heap allocator, called automatically on first `alloc`/`realloc`.
  function HeapAllocatorT:init(): void
    self.heap = {}
    self.heap:add_memory_region(&self.buffer[0], SIZE)
    self.initialized = true
  end

  --[[
  Allocates `size` bytes and returns a pointer to the allocated memory block.

  The allocated memory is not initialized.
  If `size` is zero or the operation fails, then returns `nilptr`.
  ]]
  function HeapAllocatorT:alloc(size: usize, flags: facultative(usize)): pointer
    if unlikely(not self.initialized) then self:init() end
    local p: pointer = self.heap:alloc(size)
    return p
  end

  --[[
  Deallocates the allocated memory block pointed by `p`.

  If `p` is `nilptr`, then no operation is performed.
  The `dealloc(p)` has been already been called before, then undefined behavior occurs.
  Unless `p` is `nilptr`,
  it must have been returned by an earlier allocation call from this allocator.
  ]]
  function HeapAllocatorT:dealloc(p: pointer): void
    self.heap:dealloc(p)
  end

  -- Deallocate all allocations from the heap.
  function HeapAllocatorT:deallocall(): void
    self.heap = {}
    self.initialized = false
  end

  --[[
  Changes the size of the memory block pointer by `p` from size `oldsize` bytes to `newsize` bytes.

  For more details see `Allocator:realloc`.
  ]]
  function HeapAllocatorT:realloc(p: pointer, newsize: usize, oldsize: usize): pointer
    if unlikely(not self.initialized) then self:init() end
    if unlikely(newsize == oldsize) then return p end
    p = self.heap:realloc(p, newsize)
    return p
  end

  require 'allocators.allocator'

  ## Allocator_implement_interface(HeapAllocatorT)

  ## return HeapAllocatorT
## end

--[[
Generic used to instantiate a heap allocator type in the form of `HeapAllocator(SIZE)`.

Argument `SIZE` is the size of the heap in bytes.
]]
global HeapAllocator: type = #[generalize(make_HeapAllocatorT)]#

return HeapAllocator
