--[[
The arena allocator, sometimes also known as linear, monotonic or region allocator,
allocates everything from a fixed size contiguous buffer by incrementing
an offset every new allocation.

The purpose of this allocator is to have very fast allocations with almost
no runtime cost when the maximum used space is known ahead
and to quickly deallocate many allocated objects at once with almost no runtime cost too.

Reallocations and deallocations do not free space unless once for the last recent allocation.
To free space `deallocall` should be called when all operations on its allocations are finished.

The allocator buffer will reside on the stack when declared inside a function,
or on the static memory storage when declared in a top scope,
or on the heap if allocated by the general allocator.

When declaring on the stack there is no need to perform deallocations at the end of the scope,
just leave the scope ends to have a quick cleanup.
Also take care to not use a large buffer on the stack,
or the program may crash with not enough stack space,
on some system for example the stack is limited to 1MB.

By default allocations are aligned to 8 bytes unless explicitly told otherwise.
Remember to use the proper alignment for the allocated objects to have fast memory access.

The implementation is based on
[this article](https://www.gingerbill.org/article/2019/02/08/memory-allocation-strategies-002/).
]]

require 'allocators.allocator'

-- Aligns an address.
local function align_forward(addr: usize, align: usize): usize <inline>
  return (addr + (align-1)) & ~(align-1)
end

## local function make_ArenaT(SIZE, ALIGN)
  ## ALIGN = ALIGN or 8
  ## static_assert(SIZE % ALIGN == 0, 'size must be multiple of align')
  ## static_assert(ALIGN & (ALIGN-1) == 0, 'align must be a power of two')

  local SIZE <comptime> = #[SIZE]#
  local ALIGN <comptime> = #[ALIGN]#

  -- Arena allocator record defined when instantiating the generic `ArenaAllocator`.
  local ArenaAllocatorT: type = @record{
    prev_offset: usize,
    curr_offset: usize,
    buffer: [SIZE]byte
  }

  --[[
  Allocates `size` bytes and returns a pointer to the allocated memory block,
  advancing the internal arena offset.

  The allocated memory is not initialized.
  If `size` is zero or the operation fails, then returns `nilptr`.
  ]]
  function ArenaAllocatorT:alloc(size: usize, flags: facultative(usize)): pointer
    local base: usize = (@usize)(&self.buffer[0])
    local offset: usize = align_forward(base + self.curr_offset, ALIGN) - base
    local next_offset: usize = offset + size
    if unlikely(next_offset > SIZE) then
      return nilptr
    end
    local p: pointer = &self.buffer[offset]
    self.prev_offset = offset
    self.curr_offset = next_offset
    return p
  end

  --[[
  Deallocates the allocated memory block pointed by `p`.

  If `p` is the very last allocation,
  then the internal arena offset is rewind by one allocation.
  ]]
  function ArenaAllocatorT:dealloc(p: pointer): void
    if unlikely(p == nilptr) then return end
    -- get offset for this pointer
    local offset: usize = (@usize)(p) - (@usize)(&self.buffer[0])
    check(offset < SIZE and (@usize)(p) & (ALIGN-1) == 0,  'invalid pointer')
    -- we can only dealloc the most recent allocation once
    -- any other allocation we can do nothing about
    if likely(offset == self.prev_offset) then
      self.curr_offset = offset
    end
  end

  --[[
  Deallocate all allocations.
  rewinding the entire internal arena offset.

  This operation is fast.
  ]]
  function ArenaAllocatorT:deallocall(): void
    self.prev_offset = 0
    self.curr_offset = 0
  end

  --[[
  Changes the size of the memory block pointer by `p` from size `oldsize` bytes to `newsize` bytes.

  If `p` is not the very last allocation,
  then its contents are copied to a new memory block.
  For more `realloc` details see also `Allocator:realloc`.
  ]]
  function ArenaAllocatorT:realloc(p: pointer, newsize: usize, oldsize: usize): pointer
    if unlikely(p == nilptr) then
      return self:alloc(newsize)
    elseif unlikely(newsize == 0) then
      self:dealloc(p)
      return nilptr
    end
    local offset: usize = (@usize)(p) - (@usize)(&self.buffer[0])
    check(offset < SIZE and (@usize)(p) & (ALIGN-1) == 0,  'invalid pointer')
    if likely(offset == self.prev_offset) then -- is the very last allocation?
      -- we can just update the offset here to grow or shrink
      local next_offset: usize = offset + newsize
      if unlikely(next_offset > SIZE) then
        return nilptr
      end
      self.curr_offset = next_offset
      return p
    elseif newsize > oldsize then -- growing
      -- when growing we need to move to a new allocation
      if unlikely(newsize == 0) then return nilptr end
      local newp: pointer = self:alloc(newsize)
      if likely(newp ~= nilptr and p ~= nilptr and oldsize ~= 0) then
        -- copy the mem to the new location
        memory.copy(newp, p, oldsize)
      end
      -- no dealloc is done on old pointer because it's not possible in this allocator
      return newp
    else -- same size or shrinking, can return the same pointer
      return p
    end
  end

  ## Allocator_implement_interface(ArenaAllocatorT)

  ## return ArenaAllocatorT
## end

--[[
Generic used to instantiate a arena allocator type in the form of `ArenaAllocator(SIZE, ALIGN)`.

Argument `SIZE` is the arena fixed buffer size in bytes, must be multiple of `ALIGN`.
Argument `ALIGN` is the default alignment for new allocations,
must be at least 4 and in power of two, in case absent then `8` is used.
]]
global ArenaAllocator: type = #[generalize(make_ArenaT)]#

return ArenaAllocator
